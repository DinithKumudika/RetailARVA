{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T13:52:16.680929Z",
     "start_time": "2025-05-01T13:52:16.673932Z"
    },
    "id": "E3nEYNNe6rTv"
   },
   "outputs": [],
   "source": [
    "\n",
    "# https://ssahuupgrad-93226.medium.com/using-llms-for-synthetic-data-generation-the-definitive-guide-78aab5f506f0\n",
    "# https://www.confident-ai.com/blog/the-definitive-guide-to-synthetic-data-generation-using-llms\n",
    "# https://www.confident-ai.com/blog/why-llm-as-a-judge-is-the-best-llm-evaluation-method\n",
    "# https://arxiv.org/abs/2304.12244"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T13:54:12.917621Z",
     "start_time": "2025-05-01T13:54:12.854010Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "5QXz0XHxjVGZ",
    "outputId": "a7032aed-7983-4e6d-fb8b-b99df242896a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the file specified.\n"
     ]
    }
   ],
   "source": [
    "pip install -U deepeval langchain langchain-community jq langchain-core 'ollama<0.4.0' langchain-ollama tqdm pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T13:50:59.941009Z",
     "start_time": "2025-05-01T13:50:58.677207Z"
    },
    "id": "i5LQcHCmkQ6p"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Vgq6R3TjWA09"
   },
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "def metadata_func(record: dict, metadata: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Merges existing metadata with the metadata extracted from the JSON file\n",
    "    under the 'product_details' key.\n",
    "    \"\"\"\n",
    "    metadata[\"product_details\"] = record.get(\"metadata\", {})\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "_JsP3KKo3f2D"
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import JSONLoader\n",
    "\n",
    "loader = JSONLoader(\n",
    "    file_path='../product_descriptions.json',\n",
    "    jq_schema=\".[]\",\n",
    "    content_key=\"content\",\n",
    "    metadata_func=metadata_func)\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ym1IBPS-VEZN",
    "outputId": "b4882afc-0700-4013-92d7-97a59c69c2b0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'C:\\\\Users\\\\dinit\\\\Documents\\\\Research\\\\Development\\\\RetailARVA\\\\notebooks\\\\product_descriptions.json', 'seq_num': 1, 'product_details': {'id': 1, 'name': 'The Ordinary Peeling Solution', 'brand': 'The Ordinary', 'category': 'Exfoliating Peel', 'price': 'LKR 6,350.00'}}, page_content='**Introducing The Ordinary Peeling Solution: A High-Strength Exfoliating Peel for Radiant Skin**\\n\\nExperience the power of a clinically formulated exfoliating peel with The Ordinary Peeling Solution, designed to improve skin texture, clear pore congestion, and target uneven skin tone. This high-strength exfoliator is packed with 30% Alpha Hydroxy Acids (AHA) and 2% Beta Hydroxy Acids (BHA), including Glycolic Acid, Salicylic Acid, Lactic Acid, Tartaric Acid, and Citric Acid.\\n\\n**Key Benefits:**\\n\\n* Improves skin texture for a smoother complexion\\n* Clears pore congestion to reduce the appearance of enlarged pores\\n* Targets uneven skin tone to reveal brighter, more radiant skin\\n\\n**How to Use:**\\n\\nFor optimal results, use The Ordinary Peeling Solution once or twice a week on dry skin. Apply evenly using your fingertips on clean and dry skin, avoiding the eye contour area. Leave the solution on for no more than 10 minutes before rinsing thoroughly with water. To minimize potential irritation, patch testing is recommended before first use. After application, be sure to follow up with a broad-spectrum sunscreen to protect your skin from sun sensitivity.\\n\\n**Suitable Skin Types:**\\n\\nThe Ordinary Peeling Solution is suitable for normal, oily, and combination skin types, addressing concerns such as dullness, uneven texture, and enlarged pores. However, it is not recommended for sensitive skin due to its high-strength exfoliating properties.\\n\\n**Important Safety Information:**\\n\\nAs with any exfoliating product, potential side effects may include sun sensitivity, tingling, redness, and irritation. The solution is free from fragrances and parabens, but individuals with sensitivities should still exercise caution. If you experience any adverse reactions, discontinue use and consult a dermatologist.\\n\\n**What Our Customers Say:**\\n\\nDon\\'t just take our word for it - our customers rave about The Ordinary Peeling Solution! With an average rating of 4.8/5 stars, reviewers have reported remarkable results, including \"skin that feels baby-soft after one use\" and a \"brightened complexion overnight.\" Expert dermatology research backs the effectiveness of this product, making it a trusted choice for those seeking a powerful exfoliating solution.\\n\\n**Product Details:**\\n\\n* Price: 6,350.00 LKR\\n* Brand: The Ordinary\\n* Category: Exfoliating Peel\\n* Natural: No\\n* Full Ingredient List: Glycolic Acid, Lactic Acid, Tartaric Acid, Citric Acid, Salicylic Acid, Sodium Hyaluronate Crosspolymer, Tasmannia Lanceolata Fruit/Leaf Extract\\n\\nExperience the transformative power of The Ordinary Peeling Solution and reveal radiant, healthy-looking skin. Order now and discover a brighter, smoother complexion!')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "BJI-1tlx3vqU"
   },
   "outputs": [],
   "source": [
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain.embeddings import OllamaEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "4Zb7InLmYfnb"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "EMBEDDING_MODEL = 'nomic-embed-text:latest'\n",
    "CHAT_MODEL = 'qwen3:30b-a3b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z5YjI2sSYNiZ",
    "outputId": "3cdb240e-3feb-4b14-ef28-14dea771a0d9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dinit\\AppData\\Local\\Temp\\ipykernel_35556\\2020933664.py:1: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  ollama_embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL)\n"
     ]
    }
   ],
   "source": [
    "ollama_embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ib411ejPY0Jc"
   },
   "outputs": [],
   "source": [
    "content = [doc.page_content for doc in docs]\n",
    "embeddings = []\n",
    "synthetic_dataset = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding Documents: 100%|██████████| 100/100 [09:13<00:00,  5.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 Embeddings generated successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for text in tqdm(content, desc=\"Embedding Documents\", total=len(content)):\n",
    "    embedding = ollama_embeddings.embed_query(text)\n",
    "    embeddings.append(embedding)\n",
    "\n",
    "print(f\"{len(embeddings)} Embeddings generated successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "BXdayWJBZLH5"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# randomly selecting a chunk of data to act as your focal anchor\n",
    "reference_index = random.randint(0, len(embeddings) - 1)\n",
    "reference_embedding = embeddings[reference_index]\n",
    "contexts = [content[reference_index]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sh-bwDtMne3B",
    "outputId": "d8b78c24-5c7a-4edc-b0d6-57f6ba82ba98"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['**Introducing Banila Co Clean It Zero Cleansing Balm: A Revolutionary Skincare Essential**\\n\\nExperience the power of a deep cleanse with the Banila Co Clean It Zero Cleansing Balm, a game-changing skincare product designed to remove makeup and impurities while softening and hydrating your skin. This innovative cleansing balm is perfect for all skin types, including oily, dry, combination, sensitive, and normal skin.\\n\\n**Key Ingredients and Benefits**\\n\\nThe Banila Co Clean It Zero Cleansing Balm features a unique blend of key ingredients, including Acerola Fruit Extract, Papaya Extract, and Bambusa Vulgaris Leaf Extract. These powerful ingredients work together to provide exfoliation, tighten pores, and hydrate the skin, leaving you with a smoother and more even-toned complexion.\\n\\n**How it Works**\\n\\nTo use, simply scoop a moderate amount of the cleansing balm and massage it onto your dry face. Add warm water to emulsify, then rinse thoroughly. For best results, use the spatula provided to avoid contamination. This product is perfect for use as the first step in a double cleansing routine, but its effectiveness means you may not need to double cleanse at all.\\n\\n**Skin Suitability and Concerns**\\n\\nThe Banila Co Clean It Zero Cleansing Balm is suitable for all skin types and addresses a range of skin concerns, including acne, dark spots/hyperpigmentation, oiliness, dryness, uneven skin tone, and large pores. Even sensitive skin can benefit from this gentle yet effective cleansing balm.\\n\\n**Safety Information**\\n\\nWhile the Banila Co Clean It Zero Cleansing Balm is generally safe to use, it does contain fragrances, which may be a concern for some individuals. As with any skincare product, be sure to review the full ingredient list and follow usage instructions carefully to minimize the risk of any adverse reactions.\\n\\n**What Our Customers Say**\\n\\nDon\\'t just take our word for it - our customers rave about the Banila Co Clean It Zero Cleansing Balm. With an average rating of 5.0/5 stars, reviewers praise its ability to leave skin feeling \"clean and soft without being oily.\" Expert reviewers agree, calling it an \"effective, gentle cleansing balm that deeply removes makeup and impurities without irritation.\"\\n\\n**Get Ready to Experience the Power of Clean Skin**\\n\\nTry the Banila Co Clean It Zero Cleansing Balm today and discover a simpler, more effective skincare routine. With its unique blend of ingredients, gentle yet powerful formula, and suitability for all skin types, this cleansing balm is sure to become a staple in your daily skincare regimen. Order now and experience the transformative power of clean, healthy-looking skin. (Price: 6,650.00 LKR)']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "hRImpW5xnt2K"
   },
   "outputs": [],
   "source": [
    "# set a similarity threshold and use cosine similarity to identify related chunks to build your context\n",
    "import numpy as np\n",
    "\n",
    "similarity_threshold = 0.9\n",
    "similar_indices = []\n",
    "for i, embedding in enumerate(embeddings):\n",
    "    product = np.dot(reference_embedding, embedding)\n",
    "    norm = np.linalg.norm(reference_embedding) * np.linalg.norm(embedding)\n",
    "    similarity = product / norm\n",
    "    if similarity >= similarity_threshold:\n",
    "        similar_indices.append(i)\n",
    "for i in similar_indices:\n",
    "    contexts.append(content[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UYa7ooctTB9t",
    "outputId": "7fe57ab0-86dc-425b-9ecb-8a4147231391"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "UwDP25dUnyYq"
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"I want you act as a copywriter. Based on the given context,\n",
    "which is list of strings, please generate a list of JSON objects\n",
    "with a `input` key. The `input` can either be a question or a\n",
    "statement that can be addressed by the given context.\n",
    "contexts:\n",
    "{contexts}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(\n",
    "    base_url='http://64.247.196.62:11434', \n",
    "    model=CHAT_MODEL, \n",
    "    temperature=0.6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "V8hp_md5rU0U"
   },
   "outputs": [],
   "source": [
    "query = llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "id": "UE83vcdQqdHA",
    "outputId": "b98ba33a-a590-41f0-856d-c4e3b8d3070c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\nOkay, I need to generate a list of JSON objects with an \"input\" key based on the given contexts. The input should be either a question or a statement that can be addressed by the context. Let me look at the contexts provided.\\n\\nFirst, the contexts are two identical paragraphs about the Banila Co Clean It Zero Cleansing Balm. So, the information is the same in both. The user wants me to create questions or statements that can be answered using this context.\\n\\nI should start by breaking down the context into different sections. The sections are: Introduction, Key Ingredients and Benefits, How it Works, Skin Suitability and Concerns, Safety Information, What Our Customers Say, and Get Ready to Experience the Power of Clean Skin. Each of these sections has specific information.\\n\\nFor each section, I can generate possible questions or statements. For example, under Key Ingredients, the ingredients are Acerola Fruit Extract, Papaya Extract, and Bambusa Vulgaris Leaf Extract. So a question could be \"What are the key ingredients in the Banila Co Clean It Zero Cleansing Balm?\" or a statement like \"The product contains Acerola Fruit Extract, Papaya Extract, and Bambusa Vulgaris Leaf Extract.\"\\n\\nSimilarly, under How it Works, the steps are scooping the balm, massaging onto dry face, adding water, rinsing, and using the spatula. So a question could be \"How do you use the Banila Co Clean It Zero Cleansing Balm?\" or a statement like \"To use, scoop a moderate amount and massage onto dry skin, then add warm water to emulsify.\"\\n\\nFor Skin Suitability, the product is suitable for all skin types and addresses various concerns. So a question might be \"Is the Banila Co Clean It Zero Cleansing Balm suitable for sensitive skin?\" or a statement like \"The product is suitable for oily, dry, combination, sensitive, and normal skin types.\"\\n\\nSafety Information mentions that it contains fragrances. So a question could be \"Does the product contain fragrances?\" or \"What safety information should users be aware of?\"\\n\\nCustomer reviews mention a 5.0/5 rating and positive feedback. So a question like \"What do customers say about the Banila Co Clean It Zero Cleansing Balm?\" or a statement about the average rating.\\n\\nThe price is mentioned as 6,650.00 LKR. So a question like \"What is the price of the Banila Co Clean It Zero Cleansing Balm?\" or a statement about the cost.\\n\\nI need to make sure that each input is either a question or a statement. Also, avoid repetition since the contexts are duplicates. Maybe generate 10-15 entries. Let me check for different aspects:\\n\\n1. Ingredients\\n2. Usage instructions\\n3. Skin types it\\'s suitable for\\n4. Skin concerns addressed\\n5. Safety (fragrances)\\n6. Customer reviews\\n7. Price\\n8. How it works (emulsifying)\\n9. Double cleansing\\n10. Benefits (exfoliation, hydration)\\n11. What makes it unique\\n12. Effectiveness\\n13. Suitability for sensitive skin\\n14. How to avoid contamination\\n15. Expert reviews\\n\\nThat\\'s 15. Let me structure each as a JSON object with \"input\" key. Make sure the statements are factual and the questions are answerable from the context. Avoid making up anything not in the context. Also, check for any specific details like the price, ingredients, steps, etc.\\n</think>\\n\\n[\\n  {\"input\": \"What are the key ingredients in the Banila Co Clean It Zero Cleansing Balm?\"},\\n  {\"input\": \"How do you use the Banila Co Clean It Zero Cleansing Balm?\"},\\n  {\"input\": \"Is the Banila Co Clean It Zero Cleansing Balm suitable for sensitive skin?\"},\\n  {\"input\": \"What skin concerns does the Banila Co Clean It Zero Cleansing Balm address?\"},\\n  {\"input\": \"Does the product contain fragrances?\"},\\n  {\"input\": \"What do customers say about the Banila Co Clean It Zero Cleansing Balm?\"},\\n  {\"input\": \"What is the price of the Banila Co Clean It Zero Cleansing Balm?\"},\\n  {\"input\": \"How does the product work when added to water?\"},\\n  {\"input\": \"Can the Banila Co Clean It Zero Cleansing Balm replace double cleansing?\"},\\n  {\"input\": \"What benefits do the key ingredients provide?\"},\\n  {\"input\": \"Why is the Banila Co Clean It Zero Cleansing Balm considered revolutionary?\"},\\n  {\"input\": \"How can users avoid contamination while using the product?\"},\\n  {\"input\": \"What do expert reviewers say about the product?\"},\\n  {\"input\": \"What is the average customer rating for the Banila Co Clean It Zero Cleansing Balm?\"},\\n  {\"input\": \"What makes the Banila Co Clean It Zero Cleansing Balm effective for all skin types?\"}\\n]'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### if you are using a reasoning model do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'input': 'What are the key ingredients in the Banila Co Clean It Zero Cleansing Balm?'}, {'input': 'How do you use the Banila Co Clean It Zero Cleansing Balm?'}, {'input': 'Is the Banila Co Clean It Zero Cleansing Balm suitable for sensitive skin?'}, {'input': 'What skin concerns does the Banila Co Clean It Zero Cleansing Balm address?'}, {'input': 'Does the product contain fragrances?'}, {'input': 'What do customers say about the Banila Co Clean It Zero Cleansing Balm?'}, {'input': 'What is the price of the Banila Co Clean It Zero Cleansing Balm?'}, {'input': 'How does the product work when added to water?'}, {'input': 'Can the Banila Co Clean It Zero Cleansing Balm replace double cleansing?'}, {'input': 'What benefits do the key ingredients provide?'}, {'input': 'Why is the Banila Co Clean It Zero Cleansing Balm considered revolutionary?'}, {'input': 'How can users avoid contamination while using the product?'}, {'input': 'What do expert reviewers say about the product?'}, {'input': 'What is the average customer rating for the Banila Co Clean It Zero Cleansing Balm?'}, {'input': 'What makes the Banila Co Clean It Zero Cleansing Balm effective for all skin types?'}]\n"
     ]
    }
   ],
   "source": [
    "import re, json\n",
    "\n",
    "no_think = re.sub(r'<think>.*?</think>', '', query.content, flags=re.DOTALL).strip()\n",
    "\n",
    "# 2. Locate the JSON array\n",
    "match = re.search(r'(\\[\\s*\\{.*\\}\\s*\\])', no_think, flags=re.DOTALL)\n",
    "if not match:\n",
    "    raise ValueError(\"Could not find JSON array in the output\")\n",
    "\n",
    "json_text = match.group(1)\n",
    "\n",
    "# 3. Parse it\n",
    "result = json.loads(json_text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "SUbSH99INOIP"
   },
   "outputs": [],
   "source": [
    "multi_context_template = \"\"\"\n",
    "I want you to rewrite the given `input` so that it requires readers to use information from all elements in `Context`\n",
    "1. `Input` should require information from all `Context` elements.\n",
    "2. `Rewritten Input` must be concise and fully answerable from `Context`.\n",
    "3. Do not use phrases like 'based on the provided context.'\n",
    "4. `Rewritten Input` should not exceed 15 words.\n",
    "Context: {context}\n",
    "Input: {original_input}\n",
    "Rewritten Input:\n",
    "\"\"\"\n",
    "reasoning_template = \"\"\"\n",
    "I want you to rewrite the given `input` so that it explicitly requests multi-step reasoning.\n",
    "1. `Rewritten Input` should require multiple logical connections or inferences.\n",
    "2. `Rewritten Input` should be concise and understandable.\n",
    "3. Do not use phrases like 'based on the provided context.'\n",
    "4. `Rewritten Input` must be fully answerable from `Context`.\n",
    "5. `Rewritten Input` should not exceed 15 words.\n",
    "Context: {context}\n",
    "Input: {original_input}\n",
    "Rewritten Input:\n",
    "\"\"\"\n",
    "hypothetical_scenario_template = \"\"\"\n",
    "I want you to rewrite the given `input` to incorporate a hypothetical or speculative scenario.\n",
    "1. `Rewritten Input` should encourage applying knowledge from `Context` to deduce outcomes.\n",
    "2. `Rewritten Input` should be concise and understandable.\n",
    "3. Do not use phrases like 'based on the provided context.'\n",
    "4. `Rewritten Input` must be fully answerable from `Context`.\n",
    "5. `Rewritten Input` should not exceed 15 words.\n",
    "Context: {context}\n",
    "Input: {original_input}\n",
    "Rewritten Input:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "vp63z84zx8nq"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "multi_context_template = PromptTemplate.from_template(multi_context_template)\n",
    "reasoning_template = PromptTemplate.from_template(reasoning_template)\n",
    "hypothetical_scenario_template = PromptTemplate.from_template(hypothetical_scenario_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T3EPKzYlyVcd",
    "outputId": "e0e28071-a899-4bc5-b2e8-f20680136821"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context', 'original_input'], input_types={}, partial_variables={}, template=\"\\nI want you to rewrite the given `input` so that it requires readers to use information from all elements in `Context`\\n1. `Input` should require information from all `Context` elements.\\n2. `Rewritten Input` must be concise and fully answerable from `Context`.\\n3. Do not use phrases like 'based on the provided context.'\\n4. `Rewritten Input` should not exceed 15 words.\\nContext: {context}\\nInput: {original_input}\\nRewritten Input:\\n\")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_context_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "G7_Xlj8uqwf1"
   },
   "outputs": [],
   "source": [
    "evolution_templates = [multi_context_template, reasoning_template, hypothetical_scenario_template]\n",
    "# Number of evolution steps to apply\n",
    "num_evolution_steps = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "GqXP_9BIw5QU"
   },
   "outputs": [],
   "source": [
    "multi_context_prompt = multi_context_template.format(context=contexts, original_input=result[0]['input'])\n",
    "reasoning_prompt = reasoning_template.format(context=contexts, original_input=result[0]['input'])\n",
    "hypothetical_scenario_prompt = hypothetical_scenario_template.format(context=contexts, original_input=result[0]['input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "id": "HBlQZkv-1wLf",
    "outputId": "fd66326d-c6fc-4374-f95e-d83a3d9ee028"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nI want you to rewrite the given `input` so that it requires readers to use information from all elements in `Context`\\n1. `Input` should require information from all `Context` elements.\\n2. `Rewritten Input` must be concise and fully answerable from `Context`.\\n3. Do not use phrases like \\'based on the provided context.\\'\\n4. `Rewritten Input` should not exceed 15 words.\\nContext: [\\'**Introducing Banila Co Clean It Zero Cleansing Balm: A Revolutionary Skincare Essential**\\\\n\\\\nExperience the power of a deep cleanse with the Banila Co Clean It Zero Cleansing Balm, a game-changing skincare product designed to remove makeup and impurities while softening and hydrating your skin. This innovative cleansing balm is perfect for all skin types, including oily, dry, combination, sensitive, and normal skin.\\\\n\\\\n**Key Ingredients and Benefits**\\\\n\\\\nThe Banila Co Clean It Zero Cleansing Balm features a unique blend of key ingredients, including Acerola Fruit Extract, Papaya Extract, and Bambusa Vulgaris Leaf Extract. These powerful ingredients work together to provide exfoliation, tighten pores, and hydrate the skin, leaving you with a smoother and more even-toned complexion.\\\\n\\\\n**How it Works**\\\\n\\\\nTo use, simply scoop a moderate amount of the cleansing balm and massage it onto your dry face. Add warm water to emulsify, then rinse thoroughly. For best results, use the spatula provided to avoid contamination. This product is perfect for use as the first step in a double cleansing routine, but its effectiveness means you may not need to double cleanse at all.\\\\n\\\\n**Skin Suitability and Concerns**\\\\n\\\\nThe Banila Co Clean It Zero Cleansing Balm is suitable for all skin types and addresses a range of skin concerns, including acne, dark spots/hyperpigmentation, oiliness, dryness, uneven skin tone, and large pores. Even sensitive skin can benefit from this gentle yet effective cleansing balm.\\\\n\\\\n**Safety Information**\\\\n\\\\nWhile the Banila Co Clean It Zero Cleansing Balm is generally safe to use, it does contain fragrances, which may be a concern for some individuals. As with any skincare product, be sure to review the full ingredient list and follow usage instructions carefully to minimize the risk of any adverse reactions.\\\\n\\\\n**What Our Customers Say**\\\\n\\\\nDon\\\\\\'t just take our word for it - our customers rave about the Banila Co Clean It Zero Cleansing Balm. With an average rating of 5.0/5 stars, reviewers praise its ability to leave skin feeling \"clean and soft without being oily.\" Expert reviewers agree, calling it an \"effective, gentle cleansing balm that deeply removes makeup and impurities without irritation.\"\\\\n\\\\n**Get Ready to Experience the Power of Clean Skin**\\\\n\\\\nTry the Banila Co Clean It Zero Cleansing Balm today and discover a simpler, more effective skincare routine. With its unique blend of ingredients, gentle yet powerful formula, and suitability for all skin types, this cleansing balm is sure to become a staple in your daily skincare regimen. Order now and experience the transformative power of clean, healthy-looking skin. (Price: 6,650.00 LKR)\\', \\'**Introducing Banila Co Clean It Zero Cleansing Balm: A Revolutionary Skincare Essential**\\\\n\\\\nExperience the power of a deep cleanse with the Banila Co Clean It Zero Cleansing Balm, a game-changing skincare product designed to remove makeup and impurities while softening and hydrating your skin. This innovative cleansing balm is perfect for all skin types, including oily, dry, combination, sensitive, and normal skin.\\\\n\\\\n**Key Ingredients and Benefits**\\\\n\\\\nThe Banila Co Clean It Zero Cleansing Balm features a unique blend of key ingredients, including Acerola Fruit Extract, Papaya Extract, and Bambusa Vulgaris Leaf Extract. These powerful ingredients work together to provide exfoliation, tighten pores, and hydrate the skin, leaving you with a smoother and more even-toned complexion.\\\\n\\\\n**How it Works**\\\\n\\\\nTo use, simply scoop a moderate amount of the cleansing balm and massage it onto your dry face. Add warm water to emulsify, then rinse thoroughly. For best results, use the spatula provided to avoid contamination. This product is perfect for use as the first step in a double cleansing routine, but its effectiveness means you may not need to double cleanse at all.\\\\n\\\\n**Skin Suitability and Concerns**\\\\n\\\\nThe Banila Co Clean It Zero Cleansing Balm is suitable for all skin types and addresses a range of skin concerns, including acne, dark spots/hyperpigmentation, oiliness, dryness, uneven skin tone, and large pores. Even sensitive skin can benefit from this gentle yet effective cleansing balm.\\\\n\\\\n**Safety Information**\\\\n\\\\nWhile the Banila Co Clean It Zero Cleansing Balm is generally safe to use, it does contain fragrances, which may be a concern for some individuals. As with any skincare product, be sure to review the full ingredient list and follow usage instructions carefully to minimize the risk of any adverse reactions.\\\\n\\\\n**What Our Customers Say**\\\\n\\\\nDon\\\\\\'t just take our word for it - our customers rave about the Banila Co Clean It Zero Cleansing Balm. With an average rating of 5.0/5 stars, reviewers praise its ability to leave skin feeling \"clean and soft without being oily.\" Expert reviewers agree, calling it an \"effective, gentle cleansing balm that deeply removes makeup and impurities without irritation.\"\\\\n\\\\n**Get Ready to Experience the Power of Clean Skin**\\\\n\\\\nTry the Banila Co Clean It Zero Cleansing Balm today and discover a simpler, more effective skincare routine. With its unique blend of ingredients, gentle yet powerful formula, and suitability for all skin types, this cleansing balm is sure to become a staple in your daily skincare regimen. Order now and experience the transformative power of clean, healthy-looking skin. (Price: 6,650.00 LKR)\\']\\nInput: What are the key ingredients in the Banila Co Clean It Zero Cleansing Balm?\\nRewritten Input:\\n'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_context_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KzkiCGx2wnDy"
   },
   "outputs": [],
   "source": [
    "# Function to perform random evolution steps\n",
    "def evolve_query(original_input, context, steps):\n",
    "    current_input = original_input\n",
    "    for _ in range(steps):\n",
    "        # Choose a random (or using custom logic) template from the list\n",
    "        chosen_template = random.choice(evolution_templates)\n",
    "        # Replace the placeholders with the current context and input\n",
    "        evolved_prompt = chosen_template.invoke({\"context\": context, \"original_input\": current_input})\n",
    "        # Update the current input with the \"Rewritten Input\" section\n",
    "        current_input = llm.invoke(evolved_prompt)\n",
    "        # commment out if not using a reasoning model\n",
    "        current_input = re.sub(r'<think>.*?</think>', '', current_input.content, flags=re.DOTALL).strip()\n",
    "    return current_input\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evolved_queries = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Input: What are the key ingredients in the Banila Co Clean It Zero Cleansing Balm?\n",
      "Evolved Query: What ingredients exfoliate and what skin concerns do they address?\n",
      "--------------------------------------------------\n",
      "Original Input: How do you use the Banila Co Clean It Zero Cleansing Balm?\n",
      "Evolved Query: How do ingredients and usage benefit diverse skin types?\n",
      "--------------------------------------------------\n",
      "Original Input: Is the Banila Co Clean It Zero Cleansing Balm suitable for sensitive skin?\n",
      "Evolved Query: Is it suitable for sensitive skin with fragrance concerns?\n",
      "--------------------------------------------------\n",
      "Original Input: What skin concerns does the Banila Co Clean It Zero Cleansing Balm address?\n",
      "Evolved Query: Which ingredients target concerns and their application steps?\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m original_input \u001b[38;5;129;01min\u001b[39;00m result:\n\u001b[32m      2\u001b[39m     \u001b[38;5;66;03m# Evolve the input by randomly selecting the evolution type\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     evolved_query = \u001b[43mevolve_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_input\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43minput\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_evolution_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m     \u001b[38;5;66;03m# evolve_query = re.sub(r'<think>.*?</think>', '', evolve_query.content, flags=re.DOTALL).strip()\u001b[39;00m\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOriginal Input: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moriginal_input[\u001b[33m'\u001b[39m\u001b[33minput\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mevolve_query\u001b[39m\u001b[34m(original_input, context, steps)\u001b[39m\n\u001b[32m      8\u001b[39m     evolved_prompt = chosen_template.invoke({\u001b[33m\"\u001b[39m\u001b[33mcontext\u001b[39m\u001b[33m\"\u001b[39m: context, \u001b[33m\"\u001b[39m\u001b[33moriginal_input\u001b[39m\u001b[33m\"\u001b[39m: current_input})\n\u001b[32m      9\u001b[39m     \u001b[38;5;66;03m# Update the current input with the \"Rewritten Input\" section\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     current_input = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevolved_prompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m     current_input = re.sub(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m<think>.*?</think>\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m, current_input.content, flags=re.DOTALL).strip()\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m current_input\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dinit\\Documents\\Research\\Development\\RetailARVA\\notebooks\\env\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:369\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    358\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    359\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    364\u001b[39m     **kwargs: Any,\n\u001b[32m    365\u001b[39m ) -> BaseMessage:\n\u001b[32m    366\u001b[39m     config = ensure_config(config)\n\u001b[32m    367\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    368\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m369\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    379\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dinit\\Documents\\Research\\Development\\RetailARVA\\notebooks\\env\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:946\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    937\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    938\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    939\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    943\u001b[39m     **kwargs: Any,\n\u001b[32m    944\u001b[39m ) -> LLMResult:\n\u001b[32m    945\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m946\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dinit\\Documents\\Research\\Development\\RetailARVA\\notebooks\\env\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:765\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    763\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    764\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m765\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    766\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    767\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    768\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    769\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    770\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    771\u001b[39m         )\n\u001b[32m    772\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    773\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dinit\\Documents\\Research\\Development\\RetailARVA\\notebooks\\env\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1011\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1009\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1010\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1011\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1015\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dinit\\Documents\\Research\\Development\\RetailARVA\\notebooks\\env\\Lib\\site-packages\\langchain_ollama\\chat_models.py:715\u001b[39m, in \u001b[36mChatOllama._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    708\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_generate\u001b[39m(\n\u001b[32m    709\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    710\u001b[39m     messages: \u001b[38;5;28mlist\u001b[39m[BaseMessage],\n\u001b[32m   (...)\u001b[39m\u001b[32m    713\u001b[39m     **kwargs: Any,\n\u001b[32m    714\u001b[39m ) -> ChatResult:\n\u001b[32m--> \u001b[39m\u001b[32m715\u001b[39m     final_chunk = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_chat_stream_with_aggregation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    716\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    717\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    718\u001b[39m     generation_info = final_chunk.generation_info\n\u001b[32m    719\u001b[39m     chat_generation = ChatGeneration(\n\u001b[32m    720\u001b[39m         message=AIMessage(\n\u001b[32m    721\u001b[39m             content=final_chunk.text,\n\u001b[32m   (...)\u001b[39m\u001b[32m    726\u001b[39m         generation_info=generation_info,\n\u001b[32m    727\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dinit\\Documents\\Research\\Development\\RetailARVA\\notebooks\\env\\Lib\\site-packages\\langchain_ollama\\chat_models.py:652\u001b[39m, in \u001b[36mChatOllama._chat_stream_with_aggregation\u001b[39m\u001b[34m(self, messages, stop, run_manager, verbose, **kwargs)\u001b[39m\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_chat_stream_with_aggregation\u001b[39m(\n\u001b[32m    644\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    645\u001b[39m     messages: \u001b[38;5;28mlist\u001b[39m[BaseMessage],\n\u001b[32m   (...)\u001b[39m\u001b[32m    649\u001b[39m     **kwargs: Any,\n\u001b[32m    650\u001b[39m ) -> ChatGenerationChunk:\n\u001b[32m    651\u001b[39m     final_chunk = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m652\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_iterate_over_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfinal_chunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfinal_chunk\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dinit\\Documents\\Research\\Development\\RetailARVA\\notebooks\\env\\Lib\\site-packages\\langchain_ollama\\chat_models.py:737\u001b[39m, in \u001b[36mChatOllama._iterate_over_stream\u001b[39m\u001b[34m(self, messages, stop, **kwargs)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_iterate_over_stream\u001b[39m(\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    732\u001b[39m     messages: \u001b[38;5;28mlist\u001b[39m[BaseMessage],\n\u001b[32m    733\u001b[39m     stop: Optional[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    734\u001b[39m     **kwargs: Any,\n\u001b[32m    735\u001b[39m ) -> Iterator[ChatGenerationChunk]:\n\u001b[32m    736\u001b[39m     is_thinking = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m737\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_chat_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    738\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    739\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdone\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dinit\\Documents\\Research\\Development\\RetailARVA\\notebooks\\env\\Lib\\site-packages\\langchain_ollama\\chat_models.py:639\u001b[39m, in \u001b[36mChatOllama._create_chat_stream\u001b[39m\u001b[34m(self, messages, stop, **kwargs)\u001b[39m\n\u001b[32m    636\u001b[39m chat_params = \u001b[38;5;28mself\u001b[39m._chat_params(messages, stop, **kwargs)\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chat_params[\u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m639\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.chat(**chat_params)\n\u001b[32m    640\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    641\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.chat(**chat_params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dinit\\Documents\\Research\\Development\\RetailARVA\\notebooks\\env\\Lib\\site-packages\\ollama\\_client.py:170\u001b[39m, in \u001b[36mClient._request.<locals>.inner\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    167\u001b[39m   e.response.read()\n\u001b[32m    168\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e.response.text, e.response.status_code) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43miter_lines\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m  \u001b[49m\u001b[43mpart\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m  \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43merr\u001b[49m\u001b[43m \u001b[49m\u001b[43m:=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43merror\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dinit\\Documents\\Research\\Development\\RetailARVA\\notebooks\\env\\Lib\\site-packages\\httpx\\_models.py:863\u001b[39m, in \u001b[36mResponse.iter_lines\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    861\u001b[39m decoder = LineDecoder()\n\u001b[32m    862\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=\u001b[38;5;28mself\u001b[39m._request):\n\u001b[32m--> \u001b[39m\u001b[32m863\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    864\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    865\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dinit\\Documents\\Research\\Development\\RetailARVA\\notebooks\\env\\Lib\\site-packages\\httpx\\_models.py:850\u001b[39m, in \u001b[36mResponse.iter_text\u001b[39m\u001b[34m(self, chunk_size)\u001b[39m\n\u001b[32m    848\u001b[39m chunker = TextChunker(chunk_size=chunk_size)\n\u001b[32m    849\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=\u001b[38;5;28mself\u001b[39m._request):\n\u001b[32m--> \u001b[39m\u001b[32m850\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbyte_content\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    851\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext_content\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbyte_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    852\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_content\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dinit\\Documents\\Research\\Development\\RetailARVA\\notebooks\\env\\Lib\\site-packages\\httpx\\_models.py:831\u001b[39m, in \u001b[36mResponse.iter_bytes\u001b[39m\u001b[34m(self, chunk_size)\u001b[39m\n\u001b[32m    829\u001b[39m chunker = ByteChunker(chunk_size=chunk_size)\n\u001b[32m    830\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=\u001b[38;5;28mself\u001b[39m._request):\n\u001b[32m--> \u001b[39m\u001b[32m831\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mraw_bytes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    832\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecoded\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_bytes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    833\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoded\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dinit\\Documents\\Research\\Development\\RetailARVA\\notebooks\\env\\Lib\\site-packages\\httpx\\_models.py:885\u001b[39m, in \u001b[36mResponse.iter_raw\u001b[39m\u001b[34m(self, chunk_size)\u001b[39m\n\u001b[32m    882\u001b[39m chunker = ByteChunker(chunk_size=chunk_size)\n\u001b[32m    884\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=\u001b[38;5;28mself\u001b[39m._request):\n\u001b[32m--> \u001b[39m\u001b[32m885\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mraw_stream_bytes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    886\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_num_bytes_downloaded\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mraw_stream_bytes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    887\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_stream_bytes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dinit\\Documents\\Research\\Development\\RetailARVA\\notebooks\\env\\Lib\\site-packages\\httpx\\_client.py:127\u001b[39m, in \u001b[36mBoundSyncStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> typing.Iterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_stream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dinit\\Documents\\Research\\Development\\RetailARVA\\notebooks\\env\\Lib\\site-packages\\httpx\\_transports\\default.py:116\u001b[39m, in \u001b[36mResponseStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> typing.Iterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_httpcore_stream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dinit\\Documents\\Research\\Development\\RetailARVA\\notebooks\\env\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:407\u001b[39m, in \u001b[36mPoolByteStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    405\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    406\u001b[39m     \u001b[38;5;28mself\u001b[39m.close()\n\u001b[32m--> \u001b[39m\u001b[32m407\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dinit\\Documents\\Research\\Development\\RetailARVA\\notebooks\\env\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:403\u001b[39m, in \u001b[36mPoolByteStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> typing.Iterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m403\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_stream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\n\u001b[32m    405\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dinit\\Documents\\Research\\Development\\RetailARVA\\notebooks\\env\\Lib\\site-packages\\httpcore\\_sync\\http11.py:342\u001b[39m, in \u001b[36mHTTP11ConnectionByteStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    340\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ShieldCancellation():\n\u001b[32m    341\u001b[39m     \u001b[38;5;28mself\u001b[39m.close()\n\u001b[32m--> \u001b[39m\u001b[32m342\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dinit\\Documents\\Research\\Development\\RetailARVA\\notebooks\\env\\Lib\\site-packages\\httpcore\\_sync\\http11.py:334\u001b[39m, in \u001b[36mHTTP11ConnectionByteStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    333\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mreceive_response_body\u001b[39m\u001b[33m\"\u001b[39m, logger, \u001b[38;5;28mself\u001b[39m._request, kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    337\u001b[39m     \u001b[38;5;66;03m# If we get an exception while streaming the response,\u001b[39;00m\n\u001b[32m    338\u001b[39m     \u001b[38;5;66;03m# we want to close the response (and possibly the connection)\u001b[39;00m\n\u001b[32m    339\u001b[39m     \u001b[38;5;66;03m# before raising that exception.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dinit\\Documents\\Research\\Development\\RetailARVA\\notebooks\\env\\Lib\\site-packages\\httpcore\\_sync\\http11.py:203\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_body\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    200\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Data):\n\u001b[32m    205\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mbytes\u001b[39m(event.data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dinit\\Documents\\Research\\Development\\RetailARVA\\notebooks\\env\\Lib\\site-packages\\httpcore\\_sync\\http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dinit\\Documents\\Research\\Development\\RetailARVA\\notebooks\\env\\Lib\\site-packages\\httpcore\\_backends\\sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for original_input in result:\n",
    "    # Evolve the input by randomly selecting the evolution type\n",
    "    evolved_query = evolve_query(original_input['input'], contexts, num_evolution_steps)\n",
    "    # evolve_query = re.sub(r'<think>.*?</think>', '', evolve_query.content, flags=re.DOTALL).strip()\n",
    "    print(f\"Original Input: {original_input['input']}\")\n",
    "    print(f\"Evolved Query: {evolved_query}\")\n",
    "    print(\"-\" * 50)\n",
    "    evolved_queries.append(evolved_query)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LU4P5QDt0WcC"
   },
   "outputs": [],
   "source": [
    "expected_output_template = \"\"\"\n",
    "I want you to generate an answer for the given `input`. This answer has to be factually aligned to the provided context.\n",
    "Context: {context}\n",
    "Input: {evolved_query}\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_outputs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for evolved_query in evolved_queries:\n",
    "    expected_output_template = PromptTemplate.from_template(expected_output_template)\n",
    "    expected_output_prompt = expected_output_template.invoke({\"context\": contexts, \"evolved_query\": evolved_query})\n",
    "    print(f\"Evolved Query: {evolved_query}\")\n",
    "    expected_output = llm.invoke(expected_output_prompt)\n",
    "    # only if using a reasoning model\n",
    "    expected_ouput = re.sub(r'<think>.*?</think>', '', expected_output.content, flags=re.DOTALL).strip()\n",
    "    print(f\"Expected Output: {expected_output}\")\n",
    "    print(\"-\" * 50)\n",
    "    expected_outputs.append(expected_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vFDtLopc5gIq",
    "outputId": "f9ba5991-b2e7-4bd5-eeb1-f9901a4d8b9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (2.10.6)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic) (4.12.2)\n"
     ]
    }
   ],
   "source": [
    "%pip install pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cJppq8RG5Wjz"
   },
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import Optional, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FNputhdF5Y0r"
   },
   "outputs": [],
   "source": [
    "class SyntheticData(BaseModel):\n",
    "\tquery: str\n",
    "\texpected_output: Optional[str]\n",
    "\tcontext: List[str]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, evolved_query in enumerate(evolved_queries):\n",
    "    synthetic_data = SyntheticData(\n",
    "\t    query=evolved_query,\n",
    "\t    expected_output=expected_outputs[i],\n",
    "\t    context=contexts\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XHFf8F5t5cFB"
   },
   "outputs": [],
   "source": [
    "synthetic_dataset.append(synthetic_data)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
